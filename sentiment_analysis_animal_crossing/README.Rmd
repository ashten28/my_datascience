---
title: "Sentiment analysis for Animal Crossing user reviews"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  render = "normal_print", echo = TRUE, cache=TRUE, message=FALSE, warning=FALSE)

options(width = 100)

library(tidyverse)
library(skimr)
library(tidymodels)
library(textrecipes)
library(vip)

# define my pallete
my_palette <-
  c(darkgrey = "#575965", grey = "#868d99", lightgrey = "#e2e2e2", yellow = "#f5cc5a", 
    white = "#fefefe", black = "#2e2528", black2 = "#0A0A0A", lighgrey2 = "#5b5b5b", green = "#3986A6")

scales::show_col(my_palette)

my_theme <-
  theme_bw() +
  theme(
      plot.background = element_rect(fill = my_palette[[3]]),
      panel.background = element_rect(fill= my_palette[[3]]),
      axis.line.x = element_line(colour = my_palette[[2]]),
      panel.border = element_blank(),
      panel.grid = element_blank()
  )

theme_set(my_theme)

update_geom_defaults("bar", list(fill = my_palette[9], colour = my_palette[9]))
update_geom_defaults("col", list(fill = my_palette[9], colour = my_palette[9]))

```

> "This report is written based on [Julia Silge](https://juliasilge.com/blog/animal-crossing/)'s blog. I try to incorporate my own thought process. "
> 
> --- humble beginner

## About

This report will look to provide insights using data exploration and machine learning models on a [tidytuesday's **Animal Crossing - New Horizons**](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-05-05) data set. 

Its worth mentioning that I have never heard of Animal Crossing and I apologize if I had been ignoring. However, good news, after I am done with report, I would have topped up my knowledge about this. 

In my own understanding Animal Crossing is game where it gives users the ability to participate in a life like simulation. Like comparable games like Minecraft, you are given tools and enviroments for you to freely explore. 

Hence, like every product created for human consumption, this games allows for users to rate their experience. The title probably already spoiled it but you guessed it! We are going to look into the those user reviews and perform a sentiment analysis. 

## Data

We are going to use the `skimr` package to look at a comprehensive summary of the data

```{r }
user_reviews <- 
  readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")

skimr::skim_tee(user_reviews, skim_fun = skim_without_charts)

```

This data set is pretty straightforward. We see that each observation (or row) represent a review made by a user. In total we have 2999 review with ach observation giving information about the 

  - user_name: reviewer's username
  - text: the review's commentary
  - date: date of which the review was posted, looks like there's 2 months worth of reviews
  - grade: numeric rating given by the user with a range from 0 to 10


## Exploratory

Lets look at the outcome's variable distribution:

```{r fig.align='center'}

user_reviews %>% 
  count(grade) %>% 
  ggplot(aes(x = grade, y = n)) +
  geom_col()


```

Off the bat, the distribution of looks weird. Both tails are quite heavy. 

However, lets give this some thought. These are review grades, hence ask yourself how often do you leave a review and when you do, why did you do it?

Most answers would be they rarely leave grades and when they do its because of extreme events such as complains or over the top experience. Applying this logic, it make sense to see high density on either side of grade scale. 

Looking at the distribution, it wont be wise to try a model grades directly, especially that this is my first time. Rather, if we can turn this into a classification problem where we split the grades in to bad or good rating. Let's say that a rating below 7 is bad. 

```{r}

reviews_parsed <- user_reviews %>%
  mutate(text = str_remove(text, "Expand$")) %>%
  mutate(rating = case_when(
    grade > 7 ~ "good",
    TRUE ~ "bad"
  ))

```

We could do a lot more on explatory analysis, but my skills are still at its infancy (especially text mining). Hence, I think we have enough to move on to modeling. 

## Modeling

First, we will set a seed for reproducibilty. 

Then we will split the reviews into a train set and test set using stratified sampling. We use stratified sampling so that we have balanced proportion of good and bad ratings in both data sets. 

```{r}

set.seed(123)
review_split <- initial_split(reviews_parsed, strata = rating)
review_train <- training(review_split)
review_test <- testing(review_split)

```

Now that we have our train set and test set, let look at their proportions of ratings

```{r fig.align = "center"}

# check the rating count in train and test set
review_train %>% 
  count(rating) %>% 
  mutate(prop = n / sum(n), split = "train") %>% 
  bind_rows(
    review_test %>% 
      count(rating) %>% 
      mutate(prop = n / sum(n), split = "test")
  ) %>% 
  select(split, rating, n, prop) %>% 
  ggplot(aes(x = rating, y = prop)) +
  geom_col() +
  facet_wrap(split~.)

```


Great, there's a good balance of ratings in both data sets. 

Next is data preprocessing. We will use `recipes` package for this, however since this project is text heavy, we will also use `textrecipes` package. 

### Data Preprocessing

From what I read, I understand in this step is where data mining, feature engineering and imputation happens. Essentially, we are trying to make existing predictors better and relevant while also add new variables that could add value. 

```{r}

library(textrecipes)

review_rec <- 
  recipe(rating ~ text, data = review_train) %>%
  step_tokenize(text) %>%
  step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text) %>%
  step_normalize(all_predictors())

review_prep <- prep(review_rec)

review_prep


```

We have a column called `text` in our data set. This column holds the review comments of ther user. AS a standlone, this column means almost nothing to a predictive model. The character strings that it holds are too long and unqiue. If we atttempt to use the column as it is, our models will most definitely turn out bad. 

This is where data preprocessing comes in. Let's try to bring out valueable column/predictors from the `text` column


Lets go through what we did above one by one
 - step_tokenize: the character string in our column `text` can be very long and most probably unique. It would make more sense to cut up the string into smaller strings and then we identify numerically if it exists in an oberservation. here we cut up the string into words. In other words, one token = one word. 
 - step_stopwords: we use this to remove any tokens that are stop words which give no or very low predicting power such as "is, it, there". 
 - step_tokenfilter: if the string is very long, it likely that many token will be created. The longer the string, the greater the number of columns and hence the greater number of predictors. Unless you have loads of RAM to spare, it wont be a good idea to include too many predictors. Hence, in here we limit the number of tokens to 500. 
 - step_tf: tf stands for term frequency (counts). this measures how many times each token appears in the `text` column. For instance. when our weight_scheme is set to binary, if the token "yes" appears once, then it returns a 1. If it appears twice, then it returns a 2. If there is any matching ones, then it returns a 0. Pretty neat I must say. But we can take this up a notch. 
 - step_tfidf: tfidf stands for "term frequency inverse document frequency". It holds the same principle as above but it incorporates how common or rare the token is among all the observation in the returned values. 
- step_normalize: this is also known as centering your data. What this does it takes the average of all the values and subtracts it from the data. 

Now we have our data read for modeling

### Modeling

For our model, we are going to use a lasso regression using the `glmnet` package. I am quite there are other models we can use, however since this was suggested, I am going for this one. 

```{r}

lasso_spec <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_wf <- workflow() %>%
  add_recipe(review_rec) %>%
  add_model(lasso_spec)

lasso_wf


```

You will notice that we have not set the penalty to value. Penalty is a hyperparameter, which means it is a paramter that the model cannot learn but itself and rather would rely on a hardcoded input. 

Since guessing the value would be bad (especially that am a newbie), we can iteratively try different penalties to see which one produces a model with the best performance. This is called hyperparameter tuning. 

### Hyperparameter tuning

Now, to find the best penalty value, we will do a grid search where we first make a grid of all the penalty values that we want to check and then produce a model with each of those penalties. 

```{r}

lambda_grid <- grid_regular(penalty(), levels = 40)

```

With the above, we have our grid with all the penalty values. Before we perform the tuning, we can perform resampling to get mutiple datasets. 

Here we will use bootstrap resampling. This method creates a specified n number of dataset with the same number of observation as the original data set. Keep in mind this sampling is done while using replacement. 

In plain words, when you are sampling to create a new dataset, before you randomly choose pick the next observation, you must return the last picked observation back where it will it possible to be repicked. Hence, you may have duplicates in your resampled data.  

```{r}

set.seed(123)
review_folds <- bootstraps(review_train, strata = rating)
review_folds

```

We are now ready to tune the model with grid and resampled data. 

Of course, to pick the best performing model, there must be some quantifiable metric that could give some indication. In a classification problem, the most common metric would be AUC aka area under the curve. Hence, this will be our main metric that we will use to decide the most suitable penalty value.

This process may be time consuming, hence we can use parallel computing to speed things up. 

```{r}

# doParallel::registerDoParallel()

set.seed(2020)

lasso_grid <- 
  tune_grid(
    lasso_wf,
    resamples = review_folds,
    grid = lambda_grid,
    metrics = metric_set(roc_auc, ppv, npv)
    )

```

Let's have a look at what does that look like:

```{r}

lasso_grid %>%
  collect_metrics()

```

Maybe, add some visualisation to help us see better. 

```{r}

lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap(~.metric) +
  scale_x_log10()

```

Here, we can see that AUC and PPV peaks but NPV doesn't. We could add more features to 

### Choose the best model

We will pick the best performing model based on AUC metric as our final model and fit it

```{r}

best_auc <- 
  lasso_grid %>%
  select_best("roc_auc")

final_lasso <- 
  finalize_workflow(lasso_wf, best_auc) %>% 
  fit(review_train)

final_lasso

```

Here's our final model!

One of important insight that a model can provide is variable importance. Lets look the top 20 most important predictor (in this case word) that helps predict its a positive or negative rating. 

```{r}

final_lasso %>%
  pull_workflow_fit() %>%
  vi(lambda = best_auc$penalty) %>%
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove(Variable, "tfidf_text_"),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL)

```

I am no expert in Animal Crossing, but the words you see above gives an indication of user sentiments. 

```{r, include=FALSE}
# rmarkdown::render(paste0(here::here(), '/sentiment_analysis_animal_crossing/report.Rmd'), output_format = c("github_document"))
```

